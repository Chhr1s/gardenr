dplyr::`%>%`()
devtools::check()
devtools::check()
devtools::check()
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Chunk 2: setup
#devtools::install_github("Chhr1s/gardenr")
library(gardenr)
library(glmertree)
library(dplyr)
# Chunk 3
dat <- sim_multilevel(residual_var_sd_1 = 2, residual_var_sd_2 = 6)
# Chunk 4
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 10)
# Chunk 5
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
# Chunk 6
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 2L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
# Chunk 7
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
# Chunk 8
best_fit <-
fitted %>%
arrange(mean_rmse)
best_fit
# Chunk 9
best_fit_trained <-
lmertree(
data = example_train,
formula =
ex_formula,
maxdepth = best_fit$maxdepth_par[1],
alpha = best_fit$alpha_par[1],
trim = best_fit$trim_par[1],
cluster = id_vector,
verbose = TRUE
)
# Chunk 10
plot(best_fit_trained$tree)
# Chunk 11
untrained <-
lmertree(
data = example_train,
formula =
ex_formula,
cluster = id_vector,
verbose = TRUE
)
# Chunk 12
plot(untrained$tree)
example_test %>%
mutate(
predictions_tuned =
predict(
best_fit_trained,
newdata = .,
allow.new.levels = TRUE
),
predictions_default_hyperparams =
predict(
untrained,
newdata = .,
allow.new.levels = TRUE
),
) %>%
summarize(
tuned_RMSE =
rmse(observed_y = outcome, predicted_y = predictions_tuned),
default_hyperparams_RMSE =
rmse(observed_y = outcome, predicted_y = predictions_default_hyperparams)
)
plot(untrained$tree, ep_args = list(color = 'white'))
plot(untrained$tree, ep_args = list(col = 'white'))
plot(untrained$tree, ep_args = list('white'))
plot(untrained$tree) + par(bg = 'white')
plot(untrained$tree) + par(bg = 'pink')
plot(untrained$tree, bg = 'white')
plot(untrained$tree, bg = 'black')
par(bg = "aliceblue")
plot(untrained$tree)
par(bg = "black")
plot(untrained$tree)
plot(untrained$tree)
par(bg = "black")
grid.newpage()
grid.rect(gp = gpar(col = "gray", fill = "gray"))
par(bg = "black")
plot(untrained$tree)
grid.newpage()
grid.rect(gp = gpar(col = "gray", fill = "gray"))
plot(untrained$tree)
plot(
untrained$tree,
ip_args = list(id = FALSE, fill = "gray"),
ep_args = list(fill = "gray"),
tp_args = list(id = FALSE, bg = "gray", fill = "slategray"),
newpage = FALSE
))
plot(
untrained$tree,
ip_args = list(id = FALSE, fill = "gray"),
ep_args = list(fill = "gray"),
tp_args = list(id = FALSE, bg = "gray", fill = "slategray"),
newpage = FALSE
)
plot(
untrained$tree,
ip_args = list(id = FALSE, fill = "gray"),
ep_args = list(fill = "gray"),
tp_args = list(id = FALSE, bg = "pink", fill = "slategray"),
newpage = FALSE
)
?plot
?glmertree:::plot.glmertree()
plot(
untrained$tree, 'simple')
plot(
untrained$tree, 'simple')
plot(
untrained$tree, drop_terminal = TRUE)
plot(
untrained$tree, drop_terminal = FALSE)
plot(
untrained$tree, type = 'simple') #drop_terminal = FALSE)
plot(
untrained$tree, type = 'simple', drop_terminal = FALSE)
plot(
untrained$tree, type = 'simple', drop_terminal = FALSE, fitted = 'none')
plot(
untrained,  which = 'tree.coef', type = 'simple', drop_terminal = FALSE, fitted = 'none', observed = 'false')
plot(
untrained,  which = 'tree', type = 'simple', drop_terminal = FALSE, fitted = 'none', observed = 'false')
plot(
untrained,  which = 'tree', type = 'simple', drop_terminal = FALSE, fitted = 'none', observed = 'false')
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = TRUE,
fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
#fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
#fitted = 'none',
#observed = 'false'
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
#fitted = 'none',
#observed = 'false'
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = TRUE,
fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = 'false'
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
#observed = 'false'
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree.coef',
type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree.coef',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
#fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
#observed = FALSE
)
plot(
untrained,
which = 'tree',
#type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
untrained,
which = 'tree',
type = 'simple',
drop_terminal = FALSE,
fitted = 'none',
observed = FALSE
)
plot(
best_fit_trained,
which = 'tree.coef',
)
plot(
untrained,
which = 'tree.coef',
)
?gardenr::alpha_par()
?gardenr::trim_par()
?gardenr::maxdepth_par()
?gardenr::mae()
?gardenr::rmse()
?gardenr::sim_multilevel()
?gardenr::cross_validate_it()
devtools::document()
devtools::document()
devtools::load_all()
devtools::document()
dat <- sim_multilevel(residual_var_sd_1 = 2, residual_var_sd_2 = 6)
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 2)
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 2L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
cross_validate_it <-
function(
cv_obj = cv,
seed = 713,
mod_formula = ex_formula,
tuning_grid = NULL,
#...
){
if (is.null(tuning_grid)){
tuning_grid <-
grid_max_entropy(
maxdepth_par(),
#minsize_par,
alpha_par(),
trim_par(),
#catsplit_par,
size = 25
)
message('meaningful defaults have not been implemented, please specify a tuning grid for better results')
}
set.seed(seed)
number_cv_sets <- length(cv_obj$splits)
results <- tibble()
for (j in 1:nrow(tuning_grid)){
max_depth_temp <- tuning_grid$maxdepth_par[[j]]
alpha_temp <- tuning_grid$alpha_par[[j]]
trim_temp <- tuning_grid$trim_par[[j]]
rmse_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
mae_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
for (i in 1:number_cv_sets){
temp_analysis <- analysis(cv_obj$splits[[i]])
fitted_result <-
lmertree(
data = temp_analysis,
formula = mod_formula,
maxdepth = max_depth_temp,
alpha = alpha_temp,
trim = trim_temp,
#...
)
temp_assessment <- assessment(cv_obj$splits[[i]])
temp_predictions <-
glmertree:::predict.lmertree(
fitted_result,
newdata = temp_assessment,
allow.new.levels = TRUE
)
temp_new_Y <- temp_assessment$outcome
rmse_temp[i] <- rmse(observed_y = temp_new_Y, predicted_y = temp_predictions)
mae_temp[i] <- mae(observed_y = temp_new_Y, predicted_y = temp_predictions)
message(paste0("cv index ", i, " complete"))
}
mean_rmse <- mean(rmse_temp)
mean_mae <- mean(mae_temp)
se_rmse <- sd(rmse_temp)/sqrt(length(rmse_temp))
se_mae <- sd(mae_temp)/sqrt(length(mae_temp))
temp_results <-
tuning_grid[j,] %>%
mutate(
grid_index = j,
mean_rmse = mean_rmse,
se_rmse = se_rmse,
mean_mae = mean_mae,
se_mae = se_mae,
# build in option to extract each
# fit = list(fitted_result),
) %>%
select('grid_index', everything())
results <-
bind_rows(results, temp_results)
message(paste0("hyperparameter index ", j, " complete"))
}
return(results)
}
length(cv_obj$splits)
cv_obj = cv,
seed = 713,
mod_formula = ex_formula
set.seed(seed)
number_cv_sets <- length(cv_obj$splits)
seed = 713
cv_obj = cv
mod_formula = ex_formula
set.seed(seed)
number_cv_sets <- length(cv_obj$splits)
results <- tibble()
for (j in 1:nrow(tuning_grid)){
max_depth_temp <- tuning_grid$maxdepth_par[[j]]
alpha_temp <- tuning_grid$alpha_par[[j]]
trim_temp <- tuning_grid$trim_par[[j]]
rmse_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
mae_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
for (i in 1:number_cv_sets){
temp_analysis <- analysis(cv_obj$splits[[i]])
fitted_result <-
lmertree(
data = temp_analysis,
formula = mod_formula,
maxdepth = max_depth_temp,
alpha = alpha_temp,
trim = trim_temp,
#...
)
temp_assessment <- assessment(cv_obj$splits[[i]])
temp_predictions <-
glmertree:::predict.lmertree(
fitted_result,
newdata = temp_assessment,
allow.new.levels = TRUE
)
temp_new_Y <- temp_assessment$outcome
rmse_temp[i] <- rmse(observed_y = temp_new_Y, predicted_y = temp_predictions)
mae_temp[i] <- mae(observed_y = temp_new_Y, predicted_y = temp_predictions)
message(paste0("cv index ", i, " complete"))
}
mean_rmse <- mean(rmse_temp)
mean_mae <- mean(mae_temp)
se_rmse <- sd(rmse_temp)/sqrt(length(rmse_temp))
se_mae <- sd(mae_temp)/sqrt(length(mae_temp))
temp_results <-
tuning_grid[j,] %>%
mutate(
grid_index = j,
mean_rmse = mean_rmse,
se_rmse = se_rmse,
mean_mae = mean_mae,
se_mae = se_mae,
# build in option to extract each
# fit = list(fitted_result),
) %>%
select('grid_index', everything())
results <-
bind_rows(results, temp_results)
message(paste0("hyperparameter index ", j, " complete"))
}
temp_results
results
devtools::install_github("Chhr1s/gardenr")
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site_github_pages()
usethis::use_pkgdown_github_pages()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_home()
pkgdown::build_site()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
devtools::install_github("amirmasoudabdol/preferably")
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_home()
pkgdown::build_site()
pkgdown::build_site_github_pages()
