) %>%
summarize(rmse_unseen = rmse(observed_y = outcome, predicted_y = predictions))
library(tidyverse)
example_test %>%
mutate(
predictions =
predict(
best_fit_trained,
newdata = example_test,
allow.new.levels = TRUE
)
) %>%
summarize(rmse_unseen = rmse(observed_y = outcome, predicted_y = predictions))
best_fit_trained %>%
purrr::map_dfr(broom::tidy, .id = 'node') %>%
dplyr::mutate(node = factor(node)) %>%
ggplot(
aes(
x = term,
y = estimate,
color = node,
ymin = estimate - 1.96*std.error,
ymax = estimate + 1.96*std.error
)
) +
geom_pointrange(position = position_dodge(width = 0.5)) +
theme_bw()
best_fit_trained
best_fit_trained$tree %>%
purrr::map_dfr(broom::tidy, .id = 'node') %>%
dplyr::mutate(node = factor(node)) %>%
ggplot(
aes(
x = term,
y = estimate,
color = node,
ymin = estimate - 1.96*std.error,
ymax = estimate + 1.96*std.error
)
) +
geom_pointrange(position = position_dodge(width = 0.5)) +
theme_bw()
best_fit_trained$tree
best_fit_trained$tree
plot(best_fit_trained)
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
best_fit <-
fitted %>%
dplyr::arrange(rmse)
best_fit_trained <-
lmertree(
data = example_train,
formula =
ex_formula,
maxdepth = best_fit$maxdepth_par[1],
alpha = best_fit$alpha_par[1],
trim = best_fit$trim_par[1],
cluster = id_vector,
verbose = TRUE
)
plot(best_fit_trained$tree)
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
best_fit <-
fitted %>%
dplyr::arrange(rmse)
best_fit_trained <-
lmertree(
data = example_train,
formula =
ex_formula,
maxdepth = best_fit$maxdepth_par[1],
alpha = best_fit$alpha_par[1],
trim = best_fit$trim_par[1],
cluster = id_vector,
verbose = TRUE
)
plot(best_fit_trained$tree)
best_fit_trained$tree %>%
purrr::map_dfr(broom::tidy, .id = 'node') %>%
dplyr::mutate(node = factor(node)) %>%
ggplot(
aes(
x = term,
y = estimate,
color = node,
ymin = estimate - 1.96*std.error,
ymax = estimate + 1.96*std.error
)
) +
geom_pointrange(position = position_dodge(width = 0.5)) +
theme_bw()
best_fit_trained$tree
summary(best_fit_trained$tree)
summary(best_fit_trained$tree) %>%
purrr::map_dfr(broom::tidy, .id = 'node') %>%
dplyr::mutate(node = factor(node)) %>%
ggplot(
aes(
x = term,
y = estimate,
color = node,
ymin = estimate - 1.96*std.error,
ymax = estimate + 1.96*std.error
)
) +
geom_pointrange(position = position_dodge(width = 0.5)) +
theme_bw()
usethis::use_github_action_check_standard()
usethis::use_github_action_check_standard()
usethis::use_github_action_check_standard()
usethis::use_github_action_check_standard()
use_coverage(pkg = ".", type = c("codecov"))
devtools::use_coverage(pkg = ".", type = c("codecov"))
install.packages("covr")
library(covr)
codecov(token = "2699033a-f3ce-4cdd-a449-7f8b74de5de4")
devtools::check()
install.packages("covr")
library(covr)
codecov(token = "2699033a-f3ce-4cdd-a449-7f8b74de5de4")
library(covr)
covr::codecov(token = '2699033a-f3ce-4cdd-a449-7f8b74de5de4')
install.packages('pkgdown')
install.packages("pkgdown")
usethis::use_pkgdown()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
usethis::use_pkgdown_github_pages()
pkgdown::build_site()
devtools::check()
devtools::check()
usethis::use_coverage(type = "codecov")
cross_validate_it <-
function(
cv_obj,
seed = 713,
# include_cluster = F,
mod_formula,
tuning_grid = NULL,
...
){
if (is.null(tuning_grid)){
tuning_grid <-
grid_max_entropy(
maxdepth_par(),
#minsize_par,
alpha_par(),
trim_par(),
#catsplit_par,
size = 25
)
message('meaningful defaults have not been implemented, please specify a tuning grid for better results')
}
set.seed(seed)
number_cv_sets <- length(cv_obj$splits)
results <- tibble()
for (j in 1:nrow(tuning_grid)){
max_depth_temp <- tuning_grid$maxdepth_par[[j]]
alpha_temp <- tuning_grid$alpha_par[[j]]
trim_temp <- tuning_grid$trim_par[[j]]
rmse_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
mae_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
for (i in 1:number_cv_sets){
temp_analysis <- analysis(cv_obj$splits[[i]])
fitted_result <-
lmertree(
data = temp_analysis,
formula = mod_formula,
maxdepth = max_depth_temp,
alpha = alpha_temp,
trim = trim_temp,
...
)
temp_assessment <- assessment(cv_obj$splits[[i]])
temp_predictions <-
glmertree:::predict.lmertree(
fitted_result,
newdata = temp_assessment,
allow.new.levels = TRUE
)
temp_new_Y <- temp_assessment$outcome
rmse_temp[i] <- rmse(observed_y = temp_new_Y, predicted_y = temp_predictions)
mae_temp[i] <- mae(observed_y = temp_new_Y, predicted_y = temp_predictions)
message(paste0("cv index ", i, " complete"))
}
mean_rmse <- mean(rmse_temp)
mean_mae <- mean(mae_temp)
se_rmse <- sd(rmse_temp)/sqrt(length(rmse_temp))
se_mae <- sd(mae_temp)/sqrt(length(mae_temp))
temp_results <-
tuning_grid[i,] %>%
mutate(
cv_index = i,
rmse = rmse,
mae = mae,
#fit = list(fitted_result),
) %>%
# this .data important to not get note about global
select('cv_index', everything())
results <-
bind_rows(results, temp_results)
message(paste0('iteration ', i, ' complete'))
}
return(results)
}
devtools::document()
devtools::document()
devtools::document()
devtools::install_github("Chhr1s/gardenr")
library(gardenr)
library(glmertree)
library(tidyverse)
dat <- sim_multilevel()
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 10)
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 0L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
devtools::document()
devtools::install_github("Chhr1s/gardenr")
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
# Chunk 2: example
library(gardenr)
library(glmertree)
library(tidyverse)
# Chunk 3
dat <- sim_multilevel()
# Chunk 4
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 10)
# Chunk 5
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
# Chunk 6
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 0L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
# Chunk 7
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
# Chunk 8
best_fit <-
fitted %>%
dplyr::arrange(mean_rmse)
best_fit
devtools::document()
remove.packages('gardenr')
cross_validate_it <-
function(
cv_obj,
seed = 713,
mod_formula,
tuning_grid = NULL,
...
){
if (is.null(tuning_grid)){
tuning_grid <-
grid_max_entropy(
maxdepth_par(),
#minsize_par,
alpha_par(),
trim_par(),
#catsplit_par,
size = 25
)
message('meaningful defaults have not been implemented, please specify a tuning grid for better results')
}
set.seed(seed)
number_cv_sets <- length(cv_obj$splits)
results <- tibble()
for (j in 1:nrow(tuning_grid)){
max_depth_temp <- tuning_grid$maxdepth_par[[j]]
alpha_temp <- tuning_grid$alpha_par[[j]]
trim_temp <- tuning_grid$trim_par[[j]]
rmse_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
mae_temp <- vector(mode = 'numeric', length = length(number_cv_sets))
for (i in 1:number_cv_sets){
temp_analysis <- analysis(cv_obj$splits[[i]])
fitted_result <-
lmertree(
data = temp_analysis,
formula = mod_formula,
maxdepth = max_depth_temp,
alpha = alpha_temp,
trim = trim_temp,
...
)
temp_assessment <- assessment(cv_obj$splits[[i]])
temp_predictions <-
glmertree:::predict.lmertree(
fitted_result,
newdata = temp_assessment,
allow.new.levels = TRUE
)
temp_new_Y <- temp_assessment$outcome
rmse_temp[i] <- rmse(observed_y = temp_new_Y, predicted_y = temp_predictions)
mae_temp[i] <- mae(observed_y = temp_new_Y, predicted_y = temp_predictions)
message(paste0("cv index ", i, " complete"))
}
mean_rmse <- mean(rmse_temp)
mean_mae <- mean(mae_temp)
se_rmse <- sd(rmse_temp)/sqrt(length(rmse_temp))
se_mae <- sd(mae_temp)/sqrt(length(mae_temp))
temp_results <-
tuning_grid[i,] %>%
mutate(
cv_index = i,
mean_rmse = mean_rmse,
se_rmse = se_rmse,
mean_mae = mean_mae,
se_mae = se_mae,
# build in option to extract each
# fit = list(fitted_result),
) %>%
select('cv_index', everything())
results <-
bind_rows(results, temp_results)
message(paste0("hyperparameter index ", j, " complete"))
}
return(results)
}
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
# Chunk 2: example
library(gardenr)
library(glmertree)
library(tidyverse)
# Chunk 3
dat <- sim_multilevel()
# Chunk 4
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 10)
# Chunk 5
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
# Chunk 6
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 0L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
devtools::load_all()
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
# Chunk 1
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
# Chunk 2: example
library(gardenr)
library(glmertree)
library(tidyverse)
# Chunk 3
dat <- sim_multilevel()
# Chunk 4
example_split <- rsample::initial_split(dat)
example_train <- rsample::training(example_split)
example_test  <-  rsample::testing(example_split)
cv <- rsample::vfold_cv(data = example_train, v = 10)
# Chunk 5
ex_formula <-
Formula::as.Formula(
'outcome ~ small_1 |
(1 | id_vector) |
small_2 + small_c_1 + small_c_2 + nuisance_1a + nuisance_c_1a'
)
# Chunk 6
tuning_grid <-
dials::grid_max_entropy(
maxdepth_par(maxdepth_min = 0L, maxdepth_max = 20L),
alpha_par(alpha_min = 0.10, alpha_max = 0.001),
trim_par(trim_min = 0.01, trim_max = 0.5),
size = 10
)
tuning_grid
fitted <-
cross_validate_it(
cv_obj = cv,
seed = 713,
tuning_grid = tuning_grid,
mod_formula = ex_formula,
cluster = id_vector
)
devtools::install_github("Chhr1s/gardenr")
install.packages('knitr')
install.packages("knitr")
devtools::install_github("Chhr1s/gardenr")
devtools::document()
rm(list = c("cross_validate_it"))
devtools::document()
devtools::test()
usethis::use_vignette("intro-gardenr")
library(tidyverse)
?? %>%
example_test %>%
dplyr::mutate(
predictions =
predict(
best_fit_trained,
newdata = example_test,
allow.new.levels = TRUE
)
) %>%
dplyr::summarize(
rmse_unseen =
rmse(observed_y = outcome, predicted_y = predictions)
)
??%>%
??`%>%`
?`%>%`
dplyr::`%>%`()
devtools::check()
devtools::check()
devtools::check()
